{"paragraphs":[{"text":"%md\n\n### Enriching the Flight Data\n\nAfter the data has been prepared, the next step is to fetch the daily weather data for the required time period and merge it with the flight delay data. (We have this data stored in a CSV file. If you don't have access to the data for some reason, you can download it from <https://insightedge-gettingstarted.s3.amazonaws.com/weather2017_8.csv.zip>.) \n\n**DistributedTasks** - To merge the weather data with the flight delay data we use the DistributedTasks capability, which lets us define code that executes in a distributed fashion over the in-memory data. This feature can also leverage the performance gains from the co-location of the logic/process and the data in the same Space.\n\nAfter the data is merged, we query it to verify that the new fields were added.\n","user":"anonymous","dateUpdated":"2019-09-26T06:28:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Enriching the Flight Data</h3>\n<p>After the data has been prepared, the next step is to fetch the daily weather data for the required time period and merge it with the flight delay data. (We have this data stored in a CSV file. If you don&rsquo;t have access to the data for some reason, you can download it from <a href=\"https://insightedge-gettingstarted.s3.amazonaws.com/weather2017_8.csv.zip\">https://insightedge-gettingstarted.s3.amazonaws.com/weather2017_8.csv.zip</a>.) </p>\n<p><strong>DistributedTasks</strong> - To merge the weather data with the flight delay data we use the DistributedTasks capability, which lets us define code that executes in a distributed fashion over the in-memory data. This feature can also leverage the performance gains from the co-location of the logic/process and the data in the same Space.</p>\n<p>After the data is merged, we query it to verify that the new fields were added.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569132212523_-336217485","id":"20190826-022957_1704419410","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:28:54+0000","dateFinished":"2019-09-26T06:28:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:37176"},{"text":"%dep\n\nz.load(\"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0\")\nz.load(\"com.google.code.gson:gson:2.8.5\")\n","user":"anonymous","dateUpdated":"2019-09-26T06:28:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@76feec5d\n"}]},"apps":[],"jobName":"paragraph_1569409009077_145616743","id":"20190925-105649_2122430845","dateCreated":"2019-09-25T10:56:49+0000","dateStarted":"2019-09-26T06:28:54+0000","dateFinished":"2019-09-26T06:29:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37177"},{"text":"%define\npackage model.v1\n\nimport com.gigaspaces.metadata._\nimport com.gigaspaces.metadata.index.SpaceIndexType;\nimport java.lang\nimport scala.beans.{BeanProperty}\nimport org.insightedge.scala.annotation._\nimport org.insightedge.spark.implicits.all._\n\n\n//Describe the data as Scala Case Class\n\ncase class FlightDelaysWithWeather(\n  @BeanProperty \n  @SpaceId\n  var id: String,\n  @BeanProperty \n  @SpaceIndex\n  var carrier: String,\n  @BeanProperty \n  @SpaceIndex\n  var flight_number: String,\n  @SpaceIndex\n  @BeanProperty \n  var year: Integer,\n  @BeanProperty \n  var month: String,\n  @BeanProperty \n  var dayofMonth: String,\n  @BeanProperty \n  var dayOfWeek: String,\n  @BeanProperty \n  var crsDepTime: String,\n  @SpaceIndex\n  @BeanProperty \n  var depDelay15: java.lang.Double,\n  @BeanProperty \n  var depDelay: java.lang.Double,\n  @SpaceIndex\n  @BeanProperty \n  var origin: String,\n  @BeanProperty \n  var dest: String,\n  @BeanProperty \n  var awnd: String,\n  @BeanProperty \n  var prcp: String,\n  @BeanProperty \n  var snow: String,\n  @BeanProperty \n  var tmax: String,\n  @BeanProperty \n  var tmin: String,\n  @BeanProperty \n  var cancelled: String,\n  @BeanProperty\n  var date: Integer,\n  @BeanProperty \n  var prediction: String ) {\n  def this() = this(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null)\n  def generate_id() = {id = \"%04d:%s:%s:%s:%s:%s\".format(year, month, dayofMonth, dayOfWeek, crsDepTime, flight_number)}\n}\n","user":"anonymous","dateUpdated":"2019-09-26T06:29:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res1: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@39654e8f\n"}]},"apps":[],"jobName":"paragraph_1569409051545_-141692955","id":"20190925-105731_273091578","dateCreated":"2019-09-25T10:57:31+0000","dateStarted":"2019-09-26T06:29:09+0000","dateFinished":"2019-09-26T06:29:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37178"},{"title":"Enrich the FlightDelay data with weather data","text":"%spark\nimport org.apache.spark.SparkContext._\n\nimport spark.implicits._\n\n\nval weatherDataFrame = sqlContext.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").csv(\"/opt/gigaspaces/weather2017_8.csv\")\n\n\n//Add task for merging the weather and flight delays","user":"anonymous","dateUpdated":"2019-09-26T06:29:15+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.SparkContext._\nimport spark.implicits._\nweatherDataFrame: org.apache.spark.sql.DataFrame = [Date: int, Station: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1569132212523_258618609","id":"20190818-160913_12361618","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:29:15+0000","dateFinished":"2019-09-26T06:29:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37179"},{"text":"%spark\n\nimport org.insightedge.spark.implicits.all._\nimport org.insightedge.spark.context.InsightEdgeConfig\n\n//Change space name here if not working with default\nval ieConfig = new InsightEdgeConfig(\"demo\")\n\nsc.initializeInsightEdgeContext(ieConfig)\n","user":"anonymous","dateUpdated":"2019-09-26T06:29:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.insightedge.spark.implicits.all._\nimport org.insightedge.spark.context.InsightEdgeConfig\nieConfig: org.insightedge.spark.context.InsightEdgeConfig = InsightEdgeConfig(demo,None,None)\nres1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@7ca54a11\n"}]},"apps":[],"jobName":"paragraph_1569418436965_201427215","id":"20190925-133356_683462357","dateCreated":"2019-09-25T13:33:56+0000","dateStarted":"2019-09-26T06:29:50+0000","dateFinished":"2019-09-26T06:29:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37180"},{"title":"Register the additional Types with the data grid","text":"import model.v1._\nimport com.gigaspaces.metadata._\nimport com.gigaspaces.metadata.index.SpaceIndexType;\nimport java.lang\nimport scala.beans.{BeanProperty}\nimport org.insightedge.scala.annotation._\nimport org.insightedge.spark.implicits.all._\n\n\nval typeDescriptor: SpaceTypeDescriptor = new SpaceTypeDescriptorBuilder(\"Weather\").idProperty(\"id\", true)\n                .addFixedProperty(\"Date\", \"java.lang.String\")\n                .addFixedProperty(\"Station\", \"java.lang.String\")\n                .addPropertyIndex(\"Date\", SpaceIndexType.EQUAL)\n                .addPropertyIndex(\"Station\", SpaceIndexType.EQUAL)\n                .routingProperty(\"Date\")\n                .create();\n                \n        // Register type:\nsc.grid.getTypeManager().registerTypeDescriptor(typeDescriptor)\nsc.grid.getTypeManager.registerTypeDescriptor(classOf[FlightDelaysWithWeather])\n","user":"anonymous","dateUpdated":"2019-09-26T06:29:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2019-09-26 06:29:53,833 CONFIG [com.gigaspaces.logger] - Log file: /opt/gigaspaces/logs/2019-09-26~06.29-gigaspaces-service-10.4.1.245-2631.log\nimport model.v1._\nimport com.gigaspaces.metadata._\nimport com.gigaspaces.metadata.index.SpaceIndexType\nimport java.lang\nimport scala.beans.BeanProperty\nimport org.insightedge.scala.annotation._\nimport org.insightedge.spark.implicits.all._\ntypeDescriptor: com.gigaspaces.metadata.SpaceTypeDescriptor = TypeDesc[typeName=Weather, checksum=2012354847, codebase=null, superTypesNames=[Weather, java.lang.Object], supportsDynamicProperties=true, supportsOptimisticLocking=false, systemType=false, replicatable=true, blobstoreEnabled=true, storageType=OBJECT, fifoSupport=OFF, idPropertyName=id, idAutoGenerate=true, routingPropertyName=Date, fifoGroupingPropertyName=null, sequenceNumberPropertyName=null, objectClass=, documentWrapperClass=com.gigaspaces.document.SpaceDocument, fixedProperties=[Prope..."}]},"apps":[],"jobName":"paragraph_1569132212524_6434100","id":"20190827-234813_998795403","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:29:51+0000","dateFinished":"2019-09-26T06:29:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37181"},{"title":"Write the weather data to memory","text":"%spark\nimport org.apache.spark.sql.SaveMode\n\nsc.grid.clear(\"Weather\")\nweatherDataFrame.write.mode(SaveMode.Overwrite).grid(\"Weather\")","user":"anonymous","dateUpdated":"2019-09-26T06:29:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.SaveMode\n"}]},"apps":[],"jobName":"paragraph_1569132212524_-90377343","id":"20190827-235037_392240904","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:29:55+0000","dateFinished":"2019-09-26T06:29:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37182"},{"title":"Define a task to enrich the flights delay records with relevant weather metrics","text":"%spark\nimport  model.v1._\nimport com.gigaspaces.async.AsyncResult\nimport org.openspaces.core.executor.DistributedTask\nimport org.insightedge.scala.annotation._\nimport scala.collection.JavaConversions._\nimport org.slf4j.{Logger, LoggerFactory}\nimport org.openspaces.core.GigaSpace\nimport org.openspaces.core.executor.TaskGigaSpace\nimport scala.beans.{BeanProperty}\nimport com.gigaspaces.document.SpaceDocument\nimport scala.collection.mutable.ListBuffer\nimport com.j_spaces.core.client.SQLQuery\n\n\nclass MergeTask extends DistributedTask[java.lang.Integer, java.lang.Integer] {\n  private val logger = LoggerFactory.getLogger(classOf[MergeTask])\n  @TaskGigaSpace @transient private val gs: GigaSpace = null\n\noverride def execute(): Integer = {\n    if(gs != null) {\n      val clusteredProxy = gs.getClustered()\n      logger.info(\"execute returned from: \" + gs.getSpaceName)\n        val template =new SQLQuery[FlightDelaysWithWeather](classOf[FlightDelaysWithWeather], \"tmin is null\")\n        val weatherTemplate =new SpaceDocument(\"Weather\");\n        var writtenCount:Integer = 0\n        var retArray:Array[FlightDelaysWithWeather] = null\n        logger.info(\"Created template\")\n        try {\n            while ( {retArray = gs.takeMultiple(template, 1000);retArray  != null && retArray.length > 0}) {\n                var writeCollection = new ListBuffer[FlightDelaysWithWeather]\n                for (rec <- retArray) {\n                  val dateField:Int = (\"%d%02d%02d\".format(rec.year.toInt, rec.month.toInt, rec.dayofMonth.toInt)).toInt\n                  val origin:String =  rec.origin\n                  weatherTemplate.setProperty(\"Date\", dateField)\n                  weatherTemplate.setProperty(\"Station\", origin)\n                  val weather = clusteredProxy.read(weatherTemplate)\n                  if (weather != null) {\n                    rec.awnd = \"\" + weather.getProperty(\"AWND\")\n                    rec.prcp = \"\" + weather.getProperty(\"PRCP\")\n                    rec.snow = \"\" + weather.getProperty(\"SNOW\")\n                    rec.tmin = \"\" + weather.getProperty(\"TMIN\")\n                    rec.tmax = \"\" + weather.getProperty(\"TMAX\")\n                    rec.date =  dateField\n                    writeCollection += rec\n                  }\n                }\n                if(writeCollection.length > 0) {\n                    gs.writeMultiple(writeCollection.toArray)\n                } \n                writtenCount += writeCollection.length\n            }\n        } catch {\n            case e: Exception => logger.warn(e.getMessage, e)\n        }\n      logger.info(\"Finished task \" + writtenCount)\n\n      return writtenCount\n    } else {\n      logger.info(\"gs Proxy not available\")\n        return -1\n        \n    }\n  }\n  override def reduce(results: java.util.List[AsyncResult[Integer]]): java.lang.Integer = {\n    logger.info(\"In Reduce\")\n    return results.map(_.getResult().intValue()).sum\n  }}\n","user":"anonymous","dateUpdated":"2019-09-26T06:29:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import model.v1._\nimport com.gigaspaces.async.AsyncResult\nimport org.openspaces.core.executor.DistributedTask\nimport org.insightedge.scala.annotation._\nimport scala.collection.JavaConversions._\nimport org.slf4j.{Logger, LoggerFactory}\nimport org.openspaces.core.GigaSpace\nimport org.openspaces.core.executor.TaskGigaSpace\nimport scala.beans.BeanProperty\nimport com.gigaspaces.document.SpaceDocument\nimport scala.collection.mutable.ListBuffer\nimport com.j_spaces.core.client.SQLQuery\ndefined class MergeTask\n"}]},"apps":[],"jobName":"paragraph_1569401411681_1667034538","id":"20190925-085011_609572142","dateCreated":"2019-09-25T08:50:11+0000","dateStarted":"2019-09-26T06:29:58+0000","dateFinished":"2019-09-26T06:30:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37183"},{"title":"Execute the distributed task and get the enriched data as a Spark dataframe","text":"%spark\nimport com.gigaspaces.async.AsyncFuture;\nimport org.insightedge.spark.implicits.all._\n\nval future:AsyncFuture[Integer] = sc.grid.execute(new MergeTask())\nval result = future.get()\n\n","user":"anonymous","dateUpdated":"2019-09-26T06:30:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import com.gigaspaces.async.AsyncFuture\nimport org.insightedge.spark.implicits.all._\nfuture: com.gigaspaces.async.AsyncFuture[Integer] = org.openspaces.core.transaction.internal.InternalAsyncFuture@31af8e21\nresult: Integer = 525155\n"}]},"apps":[],"jobName":"paragraph_1569132212526_-1931848953","id":"20190831-224510_2052488220","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:30:01+0000","dateFinished":"2019-09-26T06:30:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37184"},{"title":"Review the enriched data","text":"%insightedge_jdbc\n\nselect carrier , origin, dest, depDelay, awnd, prcp, snow, tmin, tmax   from FlightDelaysWithWeather where awnd is not null limit 20\n","user":"anonymous","dateUpdated":"2019-09-26T06:30:25+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"carrier":"string","origin":"string","dest":"string","depDelay":"string","awnd":"string","prcp":"string","snow":"string","tmin":"string","tmax":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"carrier\torigin\tdest\tdepDelay\tawnd\tprcp\tsnow\ttmin\ttmax\nB6\tSFO\tBOS\t17.0\t28\t76\t0\t72\t111\nF9\tDEN\tRSW\t122.0\t26\t0\t0\t-55\t56\nF9\tDEN\tCID\t-7.0\t34\t0\t0\t-82\t78\nNK\tORD\tLAS\t-2.0\t28\t28\t0\t-38\t44\nNK\tDFW\tPHL\t-11.0\t34\t74\t0\t61\t106\nOO\tDFW\tLAX\t7.0\t26\t18\t0\t61\t189\nOO\tORD\tMSP\t0.0\t28\t28\t0\t-38\t44\nOO\tSFO\tRDM\t-3.0\t33\t46\t0\t67\t111\nOO\tSFO\tLAX\t158.0\t33\t46\t0\t67\t111\nOO\tORD\tICT\t-10.0\t45\t0\t0\t-71\t39\nOO\tDEN\tPSC\t50.0\t37\t0\t0\t-121\t0\nOO\tDEN\tMFR\t80.0\t37\t0\t0\t-121\t0\nOO\tDFW\tSFO\t5.0\t71\t0\t0\t39\t161\nOO\tDEN\tCOS\t82.0\t39\t25\t38\t-155\t-110\nOO\tDEN\tFCA\t-8.0\t39\t25\t38\t-155\t-110\nOO\tORD\tMKE\t-5.0\t40\t0\t0\t-88\t-27\nOO\tSFO\tASE\t0.0\t21\t0\t0\t44\t139\nOO\tSFO\tTUS\t83.0\t21\t0\t0\t44\t139\nOO\tDEN\tRDU\t1.0\t17\t0\t0\t-77\t39\nOO\tSFO\tASE\t42.0\t22\t0\t0\t44\t144\n"}]},"apps":[],"jobName":"paragraph_1569132212527_529455456","id":"20190818-171934_1385212060","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:30:25+0000","dateFinished":"2019-09-26T06:30:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37185"},{"text":"%md\n\n### Preparing Feature Vectors\n\nIn the last stage of the data processing, we use the merged data to prepare feature vectors for all the fields we want to fit in our model, and label each vector with the resulting delay. nAs mentioned earlier, a positive number is the delay (in minutes) while a negative number indicates a flight that was early.\n","user":"anonymous","dateUpdated":"2019-09-26T06:30:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Preparing Feature Vectors</h3>\n<p>In the last stage of the data processing, we use the merged data to prepare feature vectors for all the fields we want to fit in our model, and label each vector with the resulting delay. nAs mentioned earlier, a positive number is the delay (in minutes) while a negative number indicates a flight that was early.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569132212527_-2064864035","id":"20190826-023258_963848778","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:30:41+0000","dateFinished":"2019-09-26T06:30:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37186"},{"title":"Prepare the feature vector for the ML model","text":"%spark\nimport spark.implicits._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.insightedge.scala.annotation._\nimport scala.beans.{BeanProperty}\n\n\ndef parseData(vals: Array[Double]): LabeledPoint = {\n   LabeledPoint(vals(0), Vectors.dense(vals.drop(1)))\n}\n\ndef prepareFeaturesLabeledPoint(_flightDelayswithWeatherDataSet: Dataset[FlightDelaysWithWeather]): RDD[LabeledPoint] = {\n    val featuresVectors = _flightDelayswithWeatherDataSet.map((o: FlightDelaysWithWeather) => Array(o.depDelay15.doubleValue(),  o.month.toDouble, o.dayOfWeek.toDouble,\n        (\"%04d\".format(o.crsDepTime.toInt).take(2)).toDouble, o.awnd.toDouble, o.prcp.toDouble, o.tmax.toDouble, o.tmin.toDouble))    \n     featuresVectors.rdd.map(parseData)\n} \n\nval FlightDelaysWithWeatherDataframe =  spark.read.grid[FlightDelaysWithWeather].as[FlightDelaysWithWeather]\n\nval traningData = prepareFeaturesLabeledPoint(FlightDelaysWithWeatherDataframe.filter(\"Year = 2017\"))\nval validationData = prepareFeaturesLabeledPoint(FlightDelaysWithWeatherDataframe.filter(\"Year = 2018\"))\n\n\n","user":"anonymous","dateUpdated":"2019-09-26T06:30:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.insightedge.scala.annotation._\nimport scala.beans.BeanProperty\nparseData: (vals: Array[Double])org.apache.spark.mllib.regression.LabeledPoint\nprepareFeaturesLabeledPoint: (_flightDelayswithWeatherDataSet: org.apache.spark.sql.Dataset[model.v1.FlightDelaysWithWeather])org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]\nFlightDelaysWithWeatherDataframe: org.apache.spark.sql.Dataset[model.v1.FlightDelaysWithWeather] = [id: string, carrier: string ... 18 more fields]\ntraningData: org.apache.spark.rdd.RDD[org.apache.spa..."}]},"apps":[],"jobName":"paragraph_1569132212530_1726418726","id":"20190819-134542_2030099805","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:30:41+0000","dateFinished":"2019-09-26T06:30:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37187"},{"text":"%md\n\n### Training the ML Model\n\nWe are using the Random Forest model (forest of decision trees) with the Classification strategy. The 2017 data, which we prepared before, will be used to train the model.  The result is a fitted or trained model.\n\nAfter we train the model, we'll evalute it using the helper `eval_metrics` and `Metrics` class to rate each prediction as follows:\n\n* TP - True Positive\n* TN - True Negative\n* FP - False Positive\n* FN - False Negative \n\nWe can then sum it to evalute the model's accuracy.\n\nAfter the training with the 2017 data, we'll run the model with the 2018 data and decide if the accuracy satisfies our requirements.\n\n","user":"anonymous","dateUpdated":"2019-09-26T06:30:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Training the ML Model</h3>\n<p>We are using the Random Forest model (forest of decision trees) with the Classification strategy. The 2017 data, which we prepared before, will be used to train the model. The result is a fitted or trained model.</p>\n<p>After we train the model, we&rsquo;ll evalute it using the helper <code>eval_metrics</code> and <code>Metrics</code> class to rate each prediction as follows:</p>\n<ul>\n  <li>TP - True Positive</li>\n  <li>TN - True Negative</li>\n  <li>FP - False Positive</li>\n  <li>FN - False Negative</li>\n</ul>\n<p>We can then sum it to evalute the model&rsquo;s accuracy.</p>\n<p>After the training with the 2017 data, we&rsquo;ll run the model with the 2018 data and decide if the accuracy satisfies our requirements.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569132212530_-1509481850","id":"20190826-025537_806156844","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:30:51+0000","dateFinished":"2019-09-26T06:30:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37188"},{"title":"Train the ML model and save it to the data grid","text":"%spark\n\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.configuration.Strategy\nimport org.insightedge.spark.implicits.all._\n\n\nval treeStrategy = Strategy.defaultStrategy(\"Classification\")\nval numTrees = 10 \nval featureSubsetStrategy = \"auto\" // Let the algorithm choose\nval predictFlightDelaysRFModel = RandomForest.trainClassifier(traningData, treeStrategy, numTrees, featureSubsetStrategy, seed = 123)\n//predictFlightDelaysRFModel.saveToGrid(sc, \"predictFlightDelaysRFModel\")","user":"anonymous","dateUpdated":"2019-09-26T06:30:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.configuration.Strategy\nimport org.insightedge.spark.implicits.all._\ntreeStrategy: org.apache.spark.mllib.tree.configuration.Strategy = org.apache.spark.mllib.tree.configuration.Strategy@7125a5ec\nnumTrees: Int = 10\nfeatureSubsetStrategy: String = auto\npredictFlightDelaysRFModel: org.apache.spark.mllib.tree.model.RandomForestModel =\nTreeEnsembleModel classifier with 10 trees\n"}]},"apps":[],"jobName":"paragraph_1569132212531_40481042","id":"20190820-105429_26620422","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:30:52+0000","dateFinished":"2019-09-26T06:31:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37189"},{"title":"Define the performance metrics","text":"%spark\nimport org.apache.spark.rdd._\nimport org.apache.spark.rdd.RDD\n\n// Function to compute evaluation metrics\ndef eval_metrics(labelsAndPreds: RDD[(Double, Double)]) : Tuple2[Array[Double], Array[Double]] = {\n    val tp = labelsAndPreds.filter(r => r._1==1 && r._2==1).count.toDouble\n    val tn = labelsAndPreds.filter(r => r._1==0 && r._2==0).count.toDouble\n    val fp = labelsAndPreds.filter(r => r._1==1 && r._2==0).count.toDouble\n    val fn = labelsAndPreds.filter(r => r._1==0 && r._2==1).count.toDouble\n    \n    val precision = tp / (tp+fp)\n    val recall = tp / (tp+fn)\n    val F_measure = 2*precision*recall / (precision+recall)\n    val accuracy = (tp+tn) / (tp+tn+fp+fn)\n    new Tuple2(Array(tp, tn, fp, fn), Array(precision, recall, F_measure, accuracy))\n}\n\n\nclass Metrics(labelsAndPreds: RDD[(Double, Double)]) extends java.io.Serializable {\n\n    private def filterCount(lftBnd:Int,rtBnd:Int):Double = labelsAndPreds\n                                                           .map(x => (x._1.toInt, x._2.toInt))\n                                                           .filter(_ == (lftBnd,rtBnd)).count()\n\n    lazy val tp = filterCount(1,1)  // true positives\n    lazy val tn = filterCount(0,0)  // true negatives\n    lazy val fp = filterCount(0,1)  // false positives\n    lazy val fn = filterCount(1,0)  // false negatives\n\n    lazy val precision = tp / (tp+fp)\n    lazy val recall = tp / (tp+fn)\n    lazy val F1 = 2*precision*recall / (precision+recall)\n    lazy val accuracy = (tp+tn) / (tp+tn+fp+fn)\n}","user":"anonymous","dateUpdated":"2019-09-26T06:31:50+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.rdd._\nimport org.apache.spark.rdd.RDD\neval_metrics: (labelsAndPreds: org.apache.spark.rdd.RDD[(Double, Double)])(Array[Double], Array[Double])\ndefined class Metrics\n"}]},"apps":[],"jobName":"paragraph_1569132212531_-1159061363","id":"20190825-015117_1946128941","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:31:50+0000","dateFinished":"2019-09-26T06:31:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37190"},{"title":"Evaluate the model on the test data","text":"%spark\nprintln(validationData)\nval predictionsResultsComparedToActual = validationData.map { point =>\n    val prediction = predictFlightDelaysRFModel.predict(point.features)\n    (point.label, prediction)\n}\npredictionsResultsComparedToActual.map(println)\nval modelMetrics = new Metrics(predictionsResultsComparedToActual)\nprintln(\"accuracy = %.2f\"\n        .format(modelMetrics.accuracy))\n","user":"anonymous","dateUpdated":"2019-09-26T06:31:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"MapPartitionsRDD[30] at map at <console>:88\naccuracy = 0.79\npredictionsResultsComparedToActual: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[70] at map at <console>:98\nmodelMetrics: Metrics = Metrics@2a4d7a10\n"}]},"apps":[],"jobName":"paragraph_1569132212532_1605416256","id":"20190825-003059_736568661","dateCreated":"2019-09-22T06:03:32+0000","dateStarted":"2019-09-26T06:31:51+0000","dateFinished":"2019-09-26T06:32:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37191"},{"text":"%spark\nimport kafka.serializer.StringDecoder\nimport model.v1._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\nimport com.google.gson.Gson\n\n\n\n    val ssc = new StreamingContext(sc, Seconds(2))\n    val topics = \"flights\"\n    val topicsSet = topics.split(\",\").toSet\n    val kafkaParams = Map[String, String](\"metadata.broker.list\" -> System.getenv(\"KAFKA_URL\"))\n    val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n      ssc, kafkaParams, topicsSet)\nmessages.foreachRDD { rdd =>\n  rdd.toDF().createOrReplaceTempView(\"delays\")\n   val flightDelayPredictions = rdd.map{o => \n                        val gson = new Gson()\n                        val fd = gson.fromJson( o._2, classOf[FlightDelaysWithWeather])\n                        fd.awnd = \"0.0\"\n                        fd.prcp = \"0.0\"\n                        fd.tmax = \"0.0\"\n                        fd.tmin = \"0.0\"\n                        fd.generate_id()\n                        val featuresVector = Array(fd.month.toDouble, fd.dayOfWeek.toDouble,\n        (\"%04d\".format(fd.crsDepTime.toInt).take(2)).toDouble, fd.awnd.toDouble, fd.prcp.toDouble, fd.tmax.toDouble, fd.tmin.toDouble)\n                        val prediction = predictFlightDelaysRFModel.predict( Vectors.dense(featuresVector))\n                        if(prediction > 0)\n                            fd.prediction = \"Delayed\"\n                        else\n                            fd.prediction = \"OK\"\n                        fd\n   }\n   flightDelayPredictions.saveToGrid()\n\n}  \n\nssc.start\n\n","user":"anonymous","dateUpdated":"2019-09-26T06:32:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.NullPointerException\n  at org.apache.spark.streaming.kafka.KafkaCluster$SimpleConsumerConfig.<init>(KafkaCluster.scala:399)\n  at org.apache.spark.streaming.kafka.KafkaCluster$SimpleConsumerConfig$.apply(KafkaCluster.scala:434)\n  at org.apache.spark.streaming.kafka.KafkaCluster.config(KafkaCluster.scala:53)\n  at org.apache.spark.streaming.kafka.KafkaCluster.getPartitionMetadata(KafkaCluster.scala:130)\n  at org.apache.spark.streaming.kafka.KafkaCluster.getPartitions(KafkaCluster.scala:119)\n  at org.apache.spark.streaming.kafka.KafkaUtils$.getFromOffsets(KafkaUtils.scala:211)\n  at org.apache.spark.streaming.kafka.KafkaUtils$.createDirectStream(KafkaUtils.scala:484)\n  ... 77 elided\n"}]},"apps":[],"jobName":"paragraph_1569409233858_-2070858439","id":"20190925-110033_1852097161","dateCreated":"2019-09-25T11:00:33+0000","dateStarted":"2019-09-26T06:32:23+0000","dateFinished":"2019-09-26T06:32:26+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:37192"},{"text":"%insightedge_jdbc\n\nselect * from FlightDelaysWithWeather where prediction is not null\n","user":"anonymous","dateUpdated":"2019-09-25T14:03:25+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"awnd":"string","cancelled":"string","carrier":"string","crsDepTime":"string","date":"string","dayOfWeek":"string","dayofMonth":"string","depDelay":"string","depDelay15":"string","dest":"string","flight_number":"string","id":"string","month":"string","origin":"string","prcp":"string","prediction":"string","snow":"string","tmax":"string","tmin":"string","year":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"awnd\tcancelled\tcarrier\tcrsDepTime\tdate\tdayOfWeek\tdayofMonth\tdepDelay\tdepDelay15\tdest\tflight_number\tid\tmonth\torigin\tprcp\tprediction\tsnow\ttmax\ttmin\tyear\n"}]},"apps":[],"jobName":"paragraph_1569420138918_2108591948","id":"20190925-140218_230076633","dateCreated":"2019-09-25T14:02:18+0000","dateStarted":"2019-09-25T14:03:23+0000","dateFinished":"2019-09-25T14:03:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37193"},{"text":"%md\n\n\nPU with Spark streaming large time window and try to add pridicat pushdown for maximizing pref\n\nIntro per paragraph\n\n*Predicat push down \n\n*Change routing to tail_number\n* agrregation over tail_number\n\n* Use the model\n\n* Same zookeeper?\n* Feature vectore decision tree analisys\n\n*\n","user":"anonymous","dateUpdated":"2019-09-22T06:03:32+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:1: error: ';' expected but 'with' found.\nPU with Spark streaming large time window and try to add pridicat pushdown for maximizing pref\n   ^\n"}]},"apps":[],"jobName":"paragraph_1569132212533_-2034458184","id":"20190825-054141_2133619741","dateCreated":"2019-09-22T06:03:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37194"}],"name":"Getting Started/Flight Delays 2","id":"INSIGHTEDGE-GETTING-STARTED-2","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"insightedge_jdbc:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}
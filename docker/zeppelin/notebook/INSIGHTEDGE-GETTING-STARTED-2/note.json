{
  "paragraphs": [
    {
      "text": "%md\n\n### Enriching the Flight Data\n\nAfter the data has been prepared, the next step is to fetch the daily weather data for the required time period and merge it with the flight delay data. (We have this data stored in a CSV file. If you don't have access to the data for some reason, you can download it from <https://insightedge-gettingstarted.s3.amazonaws.com/weather2017_8.csv.zip>.) \n\n**DistributedTasks** - To merge the weather data with the flight delay data we use the DistributedTasks capability, which lets us define code that executes in a distributed fashion over the in-memory data. This feature can also leverage the performance gains from the co-location of the logic/process and the data in the same Space.\n\nAfter the data is merged, we query it to verify that the new fields were added.\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:29:22+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Enriching the Flight Data</h3>\n<p>After the data has been prepared, the next step is to fetch the daily weather data for the required time period and merge it with the flight delay data. (We have this data stored in a CSV file. If you don&rsquo;t have access to the data for some reason, you can download it from <a href=\"https://insightedge-gettingstarted.s3.amazonaws.com/weather2017_8.csv.zip\">https://insightedge-gettingstarted.s3.amazonaws.com/weather2017_8.csv.zip</a>.) </p>\n<p><strong>DistributedTasks</strong> - To merge the weather data with the flight delay data we use the DistributedTasks capability, which lets us define code that executes in a distributed fashion over the in-memory data. This feature can also leverage the performance gains from the co-location of the logic/process and the data in the same Space.</p>\n<p>After the data is merged, we query it to verify that the new fields were added.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212523_-336217485",
      "id": "20190826-022957_1704419410",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:29:22+0000",
      "dateFinished": "2019-09-23T12:29:22+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "focus": true,
      "$$hashKey": "object:4696"
    },
    {
      "title": "Enrich the FlightDelay data with weather data",
      "text": "%spark\nimport org.apache.spark.SparkContext._\n\nimport spark.implicits._\n\n\nval weatherDataFrame = sqlContext.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").csv(\"/opt/gigaspaces/weather2017_8.csv\")\n\n\n//Add task for merging the weather and flight delays",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:29:49+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.SparkContext._\nimport spark.implicits._\nweatherDataFrame: org.apache.spark.sql.DataFrame = [Date: int, Station: string ... 5 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212523_258618609",
      "id": "20190818-160913_12361618",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:29:49+0000",
      "dateFinished": "2019-09-23T12:29:57+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4697"
    },
    {
      "title": "Register the additional Types with the data grid",
      "text": "import model.v1._\nimport com.gigaspaces.metadata._\nimport com.gigaspaces.metadata.index.SpaceIndexType;\nimport java.lang\nimport scala.beans.{BeanProperty}\nimport org.insightedge.scala.annotation._\nimport org.insightedge.spark.implicits.all._\n\n\nval typeDescriptor: SpaceTypeDescriptor = new SpaceTypeDescriptorBuilder(\"Weather\").idProperty(\"id\", true)\n                .addFixedProperty(\"Date\", \"java.lang.String\")\n                .addFixedProperty(\"Station\", \"java.lang.String\")\n                .addPropertyIndex(\"Date\", SpaceIndexType.EQUAL)\n                .addPropertyIndex(\"Station\", SpaceIndexType.EQUAL)\n                .routingProperty(\"Date\")\n                .create();\n                \n        // Register type:\nsc.grid.getTypeManager().registerTypeDescriptor(typeDescriptor)\nsc.grid.getTypeManager.registerTypeDescriptor(classOf[FlightDelaysWithWeather])\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:30:02+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:31: error: not found: value model\n       import model.v1._\n              ^\n<console>:52: error: not found: type FlightDelaysWithWeather\n       sc.grid.getTypeManager.registerTypeDescriptor(classOf[FlightDelaysWithWeather])\n                                                             ^\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212524_6434100",
      "id": "20190827-234813_998795403",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:30:02+0000",
      "dateFinished": "2019-09-23T12:30:02+0000",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4698"
    },
    {
      "title": "Write the weather data to memory",
      "text": "%spark\nweatherDataFrame.write.mode(SaveMode.Overwrite).grid(\"Weather\")",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:30:19+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:32: error: not found: value SaveMode\n       weatherDataFrame.write.mode(SaveMode.Overwrite).grid(\"Weather\")\n                                   ^\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212524_-90377343",
      "id": "20190827-235037_392240904",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:30:19+0000",
      "dateFinished": "2019-09-23T12:30:19+0000",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4699"
    },
    {
      "title": "Define a task to enrich the flights delay records with relevant weather metrics",
      "text": "%spark\nimport  model.v1._\nimport com.gigaspaces.async.AsyncResult\nimport org.openspaces.core.executor.DistributedTask\nimport org.insightedge.scala.annotation._\nimport scala.collection.JavaConversions._\nimport org.slf4j.{Logger, LoggerFactory}\nimport org.openspaces.core.GigaSpace\nimport org.openspaces.core.executor.TaskGigaSpace\nimport scala.beans.{BeanProperty}\nimport com.gigaspaces.document.SpaceDocument\nimport scala.collection.mutable.ListBuffer\nimport com.j_spaces.core.client.SQLQuery\n\n\nclass MergeTask extends DistributedTask[java.lang.Integer, java.lang.Integer] {\n  private val logger = LoggerFactory.getLogger(classOf[MergeTask])\n  @TaskGigaSpace @transient private val gs: GigaSpace = null\n\noverride def execute(): Integer = {\n    if(gs != null) {\n      val clusteredProxy = gs.getClustered()\n      logger.info(\"execute returned from: \" + gs.getSpaceName)\n        val template =new SQLQuery[FlightDelaysWithWeather](classOf[FlightDelaysWithWeather], \"tmin is null\")\n        val weatherTemplate =new SpaceDocument(\"Weather\");\n        var writtenCount:Integer = 0\n        var retArray:Array[FlightDelaysWithWeather] = null\n        logger.info(\"Created template\")\n        try {\n            while ( {retArray = gs.takeMultiple(template, 1000);retArray  != null && retArray.length > 0}) {\n                var writeCollection = new ListBuffer[FlightDelaysWithWeather]\n                for (rec <- retArray) {\n                  val dateField:Int = (\"%d%02d%02d\".format(rec.year.toInt, rec.month.toInt, rec.dayofMonth.toInt)).toInt\n                  val origin:String =  rec.origin\n                  weatherTemplate.setProperty(\"Date\", dateField)\n                  weatherTemplate.setProperty(\"Station\", origin)\n                  val weather = clusteredProxy.read(weatherTemplate)\n                  if (weather != null) {\n                    rec.awnd = \"\" + weather.getProperty(\"AWND\")\n                    rec.prcp = \"\" + weather.getProperty(\"PRCP\")\n                    rec.snow = \"\" + weather.getProperty(\"SNOW\")\n                    rec.tmin = \"\" + weather.getProperty(\"TMIN\")\n                    rec.tmax = \"\" + weather.getProperty(\"TMAX\")\n                    rec.date =  dateField\n                    writeCollection += rec\n                  }\n                }\n                if(writeCollection.length > 0) {\n                    gs.writeMultiple(writeCollection.toArray)\n                } \n                writtenCount += writeCollection.length\n            }\n        } catch {\n            case e: Exception => logger.warn(e.getMessage, e)\n        }\n      logger.info(\"Finished task \" + writtenCount)\n\n      return writtenCount\n    } else {\n      logger.info(\"gs Proxy not available\")\n        return -1\n        \n    }\n  }\n  override def reduce(results: java.util.List[AsyncResult[Integer]]): java.lang.Integer = {\n    logger.info(\"In Reduce\")\n    return results.map(_.getResult().intValue()).sum\n  }}\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-22T06:03:32+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import model.v1._\nimport com.gigaspaces.async.AsyncResult\nimport org.openspaces.core.executor.DistributedTask\nimport org.insightedge.scala.annotation._\nimport scala.collection.JavaConversions._\nimport org.slf4j.{Logger, LoggerFactory}\nimport org.openspaces.core.GigaSpace\nimport org.openspaces.core.executor.TaskGigaSpace\nimport scala.beans.BeanProperty\nimport com.gigaspaces.document.SpaceDocument\nimport scala.collection.mutable.ListBuffer\nimport com.j_spaces.core.client.SQLQuery\ndefined class MergeTask\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212524_-825106922",
      "id": "20190831-224414_1776282136",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:3947"
    },
    {
      "title": "Execute the distributed task and get the enriched data as a Spark dataframe",
      "text": "%spark\nimport com.gigaspaces.async.AsyncFuture;\nimport org.insightedge.spark.implicits.all._\n\nval future:AsyncFuture[Integer] = sc.grid.execute(new MergeTask())\nval result = future.get()\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:30:30+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:34: error: not found: type MergeTask\n       val future:AsyncFuture[Integer] = sc.grid.execute(new MergeTask())\n                                                             ^\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212526_-1931848953",
      "id": "20190831-224510_2052488220",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:30:30+0000",
      "dateFinished": "2019-09-23T12:30:30+0000",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4700"
    },
    {
      "title": "Review the enriched data",
      "text": "%insightedge_jdbc\n\nselect carrier , origin, dest, depDelay, awnd, prcp, snow, tmin, tmax   from FlightDelaysWithWeather where awnd is not null limit 20\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:30:42+0000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "carrier": "string",
                      "origin": "string",
                      "dest": "string",
                      "depDelay": "string",
                      "awnd": "string",
                      "prcp": "string",
                      "snow": "string",
                      "tmin": "string",
                      "tmax": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "java.sql.SQLException: Error while executing SQL \"select carrier , origin, dest, depDelay, awnd, prcp, snow, tmin, tmax   from FlightDelaysWithWeather where awnd is not null limit 20\": From line 1, column 78 to line 1, column 100: Object 'FlightDelaysWithWeather' not found\n\tat org.apache.calcite.avatica.Helper.createException(Helper.java:56)\n\tat org.apache.calcite.avatica.Helper.createException(Helper.java:41)\n\tat org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:156)\n\tat org.apache.calcite.avatica.AvaticaStatement.execute(AvaticaStatement.java:209)\n\tat org.apache.commons.dbcp2.DelegatingStatement.execute(DelegatingStatement.java:291)\n\tat org.apache.commons.dbcp2.DelegatingStatement.execute(DelegatingStatement.java:291)\n\tat org.apache.zeppelin.jdbc.JDBCInterpreter.executeSql(JDBCInterpreter.java:718)\n\tat org.apache.zeppelin.jdbc.JDBCInterpreter.interpret(JDBCInterpreter.java:801)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:632)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler$JobRunner.run(ParallelScheduler.java:162)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 78 to line 1, column 100: Object 'FlightDelaysWithWeather' not found\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463)\n\tat org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:804)\n\tat org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:789)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4393)\n\tat org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:162)\n\tat org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:167)\n\tat org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:939)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:920)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:2954)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:2939)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3181)\n\tat org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)\n\tat org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:939)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:920)\n\tat org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:220)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:895)\n\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:605)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:550)\n\tat org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:264)\n\tat org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:228)\n\tat org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:784)\n\tat org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:639)\n\tat org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:609)\n\tat org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:214)\n\tat org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:603)\n\tat org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:638)\n\tat org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:149)\n\t... 16 more\nCaused by: org.apache.calcite.sql.validate.SqlValidatorException: Object 'FlightDelaysWithWeather' not found\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463)\n\tat org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:572)\n\t... 44 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212527_529455456",
      "id": "20190818-171934_1385212060",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:30:42+0000",
      "dateFinished": "2019-09-23T12:30:42+0000",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4701"
    },
    {
      "text": "%md\n\n### Preparing Feature Vectors\n\nIn the last stage of the data processing, we use the merged data to prepare feature vectors for all the fields we want to fit in our model, and label each vector with the resulting delay. nAs mentioned earlier, a positive number is the delay (in minutes) while a negative number indicates a flight that was early.\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:44:50+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Preparing Feature Vectors</h3>\n<p>In the last stage of the data processing, we use the merged data to prepare feature vectors for all the fields we want to fit in our model, and label each vector with the resulting delay. nAs mentioned earlier, a positive number is the delay (in minutes) while a negative number indicates a flight that was early.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212527_-2064864035",
      "id": "20190826-023258_963848778",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:44:50+0000",
      "dateFinished": "2019-09-23T12:44:50+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4702"
    },
    {
      "title": "Prepare the feature vector for the ML model",
      "text": "%spark\nimport spark.implicits._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.insightedge.scala.annotation._\nimport scala.beans.{BeanProperty}\n\n\ndef parseData(vals: Array[Double]): LabeledPoint = {\n   LabeledPoint(vals(0), Vectors.dense(vals.drop(1)))\n}\n\ndef prepareFeaturesLabeledPoint(_flightDelayswithWeatherDataSet: Dataset[FlightDelaysWithWeather]): RDD[LabeledPoint] = {\n    val featuresVectors = _flightDelayswithWeatherDataSet.map((o: FlightDelaysWithWeather) => Array(o.depDelay15.doubleValue(),  o.month.toDouble, o.dayOfWeek.toDouble,\n        (\"%04d\".format(o.crsDepTime.toInt).take(2)).toDouble, o.awnd.toDouble, o.prcp.toDouble, o.tmax.toDouble, o.tmin.toDouble))    \n     featuresVectors.rdd.map(parseData)\n} \n\n\n\nval traningData = prepareFeaturesLabeledPoint(FlightDelaysWithWeatherDataframe.filter(\"Year = 2017\"))\nval validationData = prepareFeaturesLabeledPoint(FlightDelaysWithWeatherDataframe.filter(\"Year = 2018\"))\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-22T06:03:32+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import spark.implicits._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.insightedge.scala.annotation._\nimport scala.beans.BeanProperty\nparseData: (vals: Array[Double])org.apache.spark.mllib.regression.LabeledPoint\nprepareFeaturesLabeledPoint: (_flightDelayswithWeatherDataSet: org.apache.spark.sql.Dataset[model.v1.FlightDelaysWithWeather])org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]\ntraningData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[88] at map at <console>:114\nvalidationData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.reg..."
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212530_1726418726",
      "id": "20190819-134542_2030099805",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4703"
    },
    {
      "text": "%md\n\n### Training the ML Model\n\nWe are using the Random Forest model (forest of decision trees) with the Classification strategy. The 2017 data, which we prepared before, will be used to train the model.  The result is a fitted or trained model.\n\nAfter we train the model, we'll evalute it using the helper `eval_metrics` and `Metrics` class to rate each prediction as follows:\n\n* TP - True Positive\n* TN - True Negative\n* FP - False Positive\n* FN - False Negative \n\nWe can then sum it to evalute the model's accuracy.\n\nAfter the training with the 2017 data, we'll run the model with the 2018 data and decide if the accuracy satisfies our requirements.\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:49:52+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Training the ML Model</h3>\n<p>We are using the Random Forest model (forest of decision trees) with the Classification strategy. The 2017 data, which we prepared before, will be used to train the model. The result is a fitted or trained model.</p>\n<p>After we train the model, we&rsquo;ll evalute it using the helper <code>eval_metrics</code> and <code>Metrics</code> class to rate each prediction as follows:</p>\n<ul>\n  <li>TP - True Positive</li>\n  <li>TN - True Negative</li>\n  <li>FP - False Positive</li>\n  <li>FN - False Negative</li>\n</ul>\n<p>We can then sum it to evalute the model&rsquo;s accuracy.</p>\n<p>After the training with the 2017 data, we&rsquo;ll run the model with the 2018 data and decide if the accuracy satisfies our requirements.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212530_-1509481850",
      "id": "20190826-025537_806156844",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:49:52+0000",
      "dateFinished": "2019-09-23T12:49:52+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4704"
    },
    {
      "title": "Train the ML model and save it to the data grid",
      "text": "%spark\n\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.configuration.Strategy\nimport org.insightedge.spark.implicits.all._\n\n\nval treeStrategy = Strategy.defaultStrategy(\"Classification\")\nval numTrees = 10 \nval featureSubsetStrategy = \"auto\" // Let the algorithm choose\nval predictFlightDelaysRFModel = RandomForest.trainClassifier(traningData, treeStrategy, numTrees, featureSubsetStrategy, seed = 123)\n//predictFlightDelaysRFModel.saveToGrid(sc, \"predictFlightDelaysRFModel\")",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:50:13+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:31: error: not found: value traningData\n       val predictFlightDelaysRFModel = RandomForest.trainClassifier(traningData, treeStrategy, numTrees, featureSubsetStrategy, seed = 123)\n                                                                     ^\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212531_40481042",
      "id": "20190820-105429_26620422",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:50:13+0000",
      "dateFinished": "2019-09-23T12:50:27+0000",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4705"
    },
    {
      "title": "Define the performance metrics",
      "text": "%spark\nimport org.apache.spark.rdd._\nimport org.apache.spark.rdd.RDD\n\n// Function to compute evaluation metrics\ndef eval_metrics(labelsAndPreds: RDD[(Double, Double)]) : Tuple2[Array[Double], Array[Double]] = {\n    val tp = labelsAndPreds.filter(r => r._1==1 && r._2==1).count.toDouble\n    val tn = labelsAndPreds.filter(r => r._1==0 && r._2==0).count.toDouble\n    val fp = labelsAndPreds.filter(r => r._1==1 && r._2==0).count.toDouble\n    val fn = labelsAndPreds.filter(r => r._1==0 && r._2==1).count.toDouble\n    \n    val precision = tp / (tp+fp)\n    val recall = tp / (tp+fn)\n    val F_measure = 2*precision*recall / (precision+recall)\n    val accuracy = (tp+tn) / (tp+tn+fp+fn)\n    new Tuple2(Array(tp, tn, fp, fn), Array(precision, recall, F_measure, accuracy))\n}\n\n\nclass Metrics(labelsAndPreds: RDD[(Double, Double)]) extends java.io.Serializable {\n\n    private def filterCount(lftBnd:Int,rtBnd:Int):Double = labelsAndPreds\n                                                           .map(x => (x._1.toInt, x._2.toInt))\n                                                           .filter(_ == (lftBnd,rtBnd)).count()\n\n    lazy val tp = filterCount(1,1)  // true positives\n    lazy val tn = filterCount(0,0)  // true negatives\n    lazy val fp = filterCount(0,1)  // false positives\n    lazy val fn = filterCount(1,0)  // false negatives\n\n    lazy val precision = tp / (tp+fp)\n    lazy val recall = tp / (tp+fn)\n    lazy val F1 = 2*precision*recall / (precision+recall)\n    lazy val accuracy = (tp+tn) / (tp+tn+fp+fn)\n}",
      "user": "anonymous",
      "dateUpdated": "2019-09-22T06:03:32+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.rdd._\nimport org.apache.spark.rdd.RDD\neval_metrics: (labelsAndPreds: org.apache.spark.rdd.RDD[(Double, Double)])(Array[Double], Array[Double])\ndefined class Metrics\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212531_-1159061363",
      "id": "20190825-015117_1946128941",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4706"
    },
    {
      "title": "Evaluate the model on the test data",
      "text": "%spark\nprintln(validationData)\nval predictionsResultsComparedToActual = validationData.map { point =>\n    val prediction = predictFlightDelaysRFModel.predict(point.features)\n    (point.label, prediction)\n}\nval modelMetrics = new Metrics(predictionsResultsComparedToActual)\nprintln(\"accuracy = %.2f\"\n        .format(modelMetrics.accuracy))\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-23T12:50:24+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:24: error: not found: value validationData\n       val predictionsResultsComparedToActual = validationData.map { point =>\n                                                ^\n<console>:25: error: not found: value predictFlightDelaysRFModel\n           val prediction = predictFlightDelaysRFModel.predict(point.features)\n                            ^\n<console>:28: error: not found: type Metrics\n       val modelMetrics = new Metrics(predictionsResultsComparedToActual)\n                              ^\n<console>:23: error: not found: value validationData\n       println(validationData)\n               ^\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212532_1605416256",
      "id": "20190825-003059_736568661",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "dateStarted": "2019-09-23T12:50:24+0000",
      "dateFinished": "2019-09-23T12:50:27+0000",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4707"
    },
    {
      "text": "%md\n\n\nPU with Spark streaming large time window and try to add pridicat pushdown for maximizing pref\n\nIntro per paragraph\n\n*Predicat push down \n\n*Change routing to tail_number\n* agrregation over tail_number\n\n* Use the model\n\n* Same zookeeper?\n* Feature vectore decision tree analisys\n\n*\n",
      "user": "anonymous",
      "dateUpdated": "2019-09-22T06:03:32+0000",
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:1: error: ';' expected but 'with' found.\nPU with Spark streaming large time window and try to add pridicat pushdown for maximizing pref\n   ^\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1569132212533_-2034458184",
      "id": "20190825-054141_2133619741",
      "dateCreated": "2019-09-22T06:03:32+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:4708"
    }
  ],
  "name": "Getting Started/Flight Delays 2",
  "id": "INSIGHTEDGE-GETTING-STARTED-2",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "insightedge_jdbc:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}
